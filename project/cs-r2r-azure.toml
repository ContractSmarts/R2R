[app]
# LLM used for internal operations (e.g., naming, summaries)
fast_llm = "azure/gpt-4o-mini"

# LLM used for RAG answers and end-user-facing chat
quality_llm = "azure/gpt-4o"

# LLM used for processing visual inputs (same as main model unless split)
vlm = "azure/gpt-4o"

# Model used for audio transcription (Whisper)
audio_lm = "azure/whisper-1"

# Models used by the research agent (e.g., multi-step reasoning)
reasoning_llm = "azure/o4-mini"
planning_llm  = "azure/o4-mini"


[embedding]
# Used for document vectorization
base_model = "azure/text-embedding-3-small"
# Optional: explicitly set embedding dimensionality (1536 for text-embedding-3-small)
# base_dimension = 1536


[completion_embedding]
# Used for embedding LLM completions (e.g., enriched summaries)
base_model = "azure/text-embedding-3-small"


[ingestion]
provider = "unstructured_local"
strategy = "auto"                         # auto-select best parser per file type
chunking_strategy = "by_title"           # keep chunks aligned with sections
new_after_n_chars = 2048                 # split chunks when title is far away
max_characters = 4096                    # upper limit for any single chunk
combine_under_n_chars = 1024            # small chunks get merged
overlap = 1024                           # soft overlap between adjacent chunks

document_summary_model = "azure/gpt-4o-mini"
automatic_extraction = true             # enable extraction of entities and relations

  [ingestion.extra_parsers]
  pdf = ["zerox", "ocr"]                # use ZeroX (PDF layer) and OCR as fallbacks

  [ingestion.chunk_enrichment_settings]
  generation_config = { model = "azure/gpt-4o-mini" }


[orchestration]
provider = "hatchet"                    # built-in orchestrator
kg_creation_concurrency_limit = 16     # reasonable parallelism for medium VMs
ingestion_concurrency_limit = 4        # align with vCPU count
kg_concurrency_limit = 4               # tune if knowledge graph step is intensive


# ------------------------------------------------------
# [database] section: Enable for rate limiting & quotas
# ------------------------------------------------------
#[database]
#  [database.limits]
#  # Per-R2R global rate limits
#  global_per_min = 60           # number of requests per minute allowed
#  monthly_limit = 10000         # total requests allowed per month
#
#  [database.route_limits]
#  "/v3/retrieval/search" = { route_per_min = 30, monthly_limit = 5000 }
#
#  # Optional: User-specific rate limiting (by user UUID)
#  [database.user_limits."47e53676-b478-5b3f-a409-234ca2164de5"]
#  global_per_min = 10
#  route_per_min = 5


# ------------------------------------------------------
# [auth] section: Optional; enable for login, access control
# ------------------------------------------------------
#[auth]
#provider = "r2r"                        # Options: r2r, oidc (experimental)
#access_token_lifetime_in_minutes = 60
#refresh_token_lifetime_in_days = 7
#require_authentication = true          # Enforce login for all endpoints
#require_email_verification = false     # Turn on if you're sending emails
#default_admin_email = "admin@example.com"
#default_admin_password = "change_me_immediately"
#
## For OIDC (e.g., Auth0, Azure AD) â€” future option
#[auth.oidc]
#issuer_url = "https://login.microsoftonline.com/your-tenant-id/v2.0"
#client_id = "your-client-id"
#client_secret = "your-client-secret"
#redirect_uri = "https://your-app-url/callback"


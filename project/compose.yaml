name: contractsmarts

# `docker compose` resolves variables at compose time from:
# 1. Shell environment variables
# 2. .env file in the same directory as the compose.yaml file
# 3. NOT from env_file entries (these are only passed to containers)

volumes:
  # (named volumes)
  postgres_data:
  minio_data:
  hatchet_rabbitmq_data:
  hatchet_certs:
  hatchet_api_key:
  
networks:
  postgres:
    name: postgres_network
    driver: bridge
  backend:
    name: backend_network
    driver: bridge
  frontend:
    name: frontend_network
    driver: bridge

services:
  # Sole postgres install service. R2R, Hatchet, etc. use the same postgres database
  postgres:
    image: pgvector/pgvector:pg16
    profiles: [step1]
    environment:
      # Superuser (admin) credentials - from .env
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      
      # Default database (will be created automatically)
      - POSTGRES_DB=postgres
      
      # PostgreSQL configuration
      - POSTGRES_MAX_CONNECTIONS=1024
      - POSTGRES_STATEMENT_CACHE_SIZE=100
      - PGPORT=5432
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - postgres
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: on-failure
    command: >
      postgres
      -c max_connections=1024
      -c shared_preload_libraries=vector

  ## after the first step, run `scripts/setup-db.sh`

  # MinIO is a self-hosted S3-compatible object storage service. It's used for:
  #   - File storage: PDFs, documents, images that R2R processes
  #   - Vector storage: Some R2R configurations use S3-compatible storage for vectors
  #   - Artifact storage: Processed documents, embeddings, etc.
  minio:
    image: minio/minio
    profiles: [step2]
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    volumes:
      - minio_data:/data
    networks:
      - backend
    ports:
      - "9000:9000"  # MinIO API (for debugging)
      - "9001:9001"  # MinIO Console (web UI for management)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: on-failure
    command: server /data --console-address ":9001"

  # Graph clustering is a processing service that:
  #  - Performs clustering analysis on document embeddings/graphs
  #  - Is called by R2R for document processing and retrieval
  #  - Is an internal computational service (not user-facing)
  #  - Needs to communicate with other backend services
  graph_clustering:
    image: ragtoriches/cluster-prod
    profiles: [step2]
    networks:
      - backend
    # Remove ports for security (R2R will access via http://graph_clustering:7276)
    ports:
      - "7276:7276"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7276/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # The unstructured service handles document parsing and processing:
  #  - Extracts text from PDFs, Word docs, PowerPoint, etc.
  #  - Handles OCR for images and scanned documents
  #  - Converts various file formats into structured text for R2R
  #  - Called by R2R during document ingestion 
  unstructured:
    image: ragtoriches/unst-prod
    profiles: [step2]
    networks:
      - backend
    # Remove ports for security (R2R will access via http://unstructured:7275)
    ports:
      - "7275:7275"  # Remove this once R2R integration is working
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7275/health"]
      interval: 10s
      timeout: 5s
      retries: 5
  
  # RabbitMQ is a message queue broker that Hatchet uses for:
  #  - Task queuing: Queuing workflow tasks to be processed
  #  - Job distribution: Distributing work across multiple workers  
  #  - Event handling: Managing workflow events and notifications
  #  - Reliability: Ensuring tasks don't get lost if workers crash
  hatchet-rabbitmq:
    image: "rabbitmq:3-management"
    profiles: [step2]
    hostname: "hatchet-rabbitmq"
    environment:
      - RABBITMQ_DEFAULT_USER=${RABBITMQ_DEFAULT_USER}
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_DEFAULT_PASS}
    volumes:
      - hatchet_rabbitmq_data:/var/lib/rabbitmq
      # Note: Using defaults for now, can add custom config later:
      # - ./conf/hatchet_rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
    networks:
      - backend
    ports:
      - "5673:5672"   # AMQP port (for development/debugging)
      - "15673:15672" # Management UI (for development/debugging)
    healthcheck:
      test: ["CMD", "rabbitmqctl", "status"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: on-failure
  
  # Hatchet migration creates the database schema (tables, indexes, etc.) that Hatchet needs to operate. This includes
  #  - Database structure setup: Creates tables for workflows, tasks, users, etc.
  #  - Schema versioning: Applies database migrations to get to the correct schema version
  #  - One-time setup: Runs once to prepare the database, then exits
  #
  # Even though we created the empty hatchet database, we still need hatchet-migration because:
  #  - Empty database: We created the database, but it has no tables yet
  #  - Schema creation: Migration creates ~20+ tables Hatchet needs
  #  - Proper structure: Creates indexes, constraints, foreign keys
  #
  # Run with `docker compose --profile setup up hatchet-migration`. It only needs to be run once.
  hatchet-migration:
    image: ghcr.io/hatchet-dev/hatchet/hatchet-migrate:v0.53.15
    profiles: [setup]
    environment:
      # Point to our shared postgres with the hatchet database
      - DATABASE_URL=postgres://${HATCHET_POSTGRES_USER}:${HATCHET_POSTGRES_PASSWORD}@postgres:5432/${HATCHET_POSTGRES_DBNAME}?sslmode=disable
    networks:
      - postgres  # Needs to connect to postgres
    depends_on:
      postgres:
        condition: service_healthy
    restart: "no"  # Run once and exit

  # Hatchet-setup-config generates initial configuration files including:
  #  - Server configuration (server.yaml) with encryption keys
  #  - Authentication settings and cookie secrets
  #  - Default tenant and user setup
  #  - RabbitMQ and database connection configuration
  #
  # Run with `docker compose --profile setup up hatchet-setup-config`. It only needs to be run once.
  hatchet-setup-config:
    image: ghcr.io/hatchet-dev/hatchet/hatchet-admin:v0.53.15
    profiles: [setup]
    command: /hatchet/hatchet-admin quickstart --skip certs --generated-config-dir /hatchet/config --overwrite=false
    environment:
      # Point to our shared infrastructure
      - DATABASE_URL=postgres://${HATCHET_POSTGRES_USER}:${HATCHET_POSTGRES_PASSWORD}@postgres:5432/${HATCHET_POSTGRES_DBNAME}?sslmode=disable
      - SERVER_TASKQUEUE_RABBITMQ_URL=amqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@hatchet-rabbitmq:5672/
    volumes:
      - hatchet_certs:/hatchet/certs 
      - ./config/hatchet:/hatchet/config  # Bind mount so you can access/edit config files
    networks:
      - postgres   # Needs database access for tenant setup
      - backend    # Needs RabbitMQ access for queue configuration
    depends_on:
      hatchet-migration:
        condition: service_completed_successfully
      hatchet-rabbitmq:
        condition: service_healthy
    restart: "no"  # Run once and exit

  # Hatchet setup-token generates API tokens for external services:
  #  - Creates authentication tokens for services like R2R
  #  - Stores tokens in shared volume for other containers to access
  #  - Enables programmatic access to Hatchet workflow APIs
  #  - Must run after config setup to use generated configuration
  #
  # Run with `docker compose --profile setup up setup-token`. It only needs to be run once.
  setup-token:
    image: ghcr.io/hatchet-dev/hatchet/hatchet-admin:v0.53.15
    profiles: [setup]
    command: sh /scripts/setup-token.sh
    environment:
      # Point to our shared infrastructure
      - DATABASE_URL=postgres://${HATCHET_POSTGRES_USER}:${HATCHET_POSTGRES_PASSWORD}@postgres:5432/${HATCHET_POSTGRES_DBNAME}?sslmode=disable
      - SERVER_TASKQUEUE_RABBITMQ_URL=amqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@hatchet-rabbitmq:5672/
    volumes:
      - ./scripts:/scripts
      - ./config/hatchet:/hatchet/config  # Read config created by setup-config
      - hatchet_certs:/hatchet/certs
      - hatchet_api_key:/hatchet_api_key  # Store generated tokens here
    networks:
      - postgres   # Needs database access to create tokens
      - backend    # May need RabbitMQ access
    depends_on:
      hatchet-setup-config:
        condition: service_completed_successfully
    restart: "no"  # Run once and exit

  # Hatchet engine is the core workflow orchestration service that:
  #  - Manages workflow execution and task distribution
  #  - Connects to PostgreSQL for state management
  #  - Uses RabbitMQ for task queuing and worker communication
  #  - Provides GRPC API on port 7077 for external services
  #  - Serves as the central workflow coordinator
  hatchet-engine:
    image: ghcr.io/hatchet-dev/hatchet/hatchet-engine:v0.53.15
    profiles: [step3]
    command: /hatchet/hatchet-engine --config /hatchet/config
    restart: on-failure
    environment:
      # Point to our shared infrastructure
      - DATABASE_URL=postgres://${HATCHET_POSTGRES_USER}:${HATCHET_POSTGRES_PASSWORD}@postgres:5432/${HATCHET_POSTGRES_DBNAME}?sslmode=disable
      - SERVER_TASKQUEUE_RABBITMQ_URL=amqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@hatchet-rabbitmq:5672/
    volumes:
      - hatchet_certs:/hatchet/certs
      - ./config/hatchet:/hatchet/config  # Use consistent path
    networks:
      - postgres   # Needs database access for workflow state
      - backend    # Needs RabbitMQ for task queuing
    ports:
      - "7077:7077"  # GRPC API for workflow management
      - "8733:8733"  # Health endpoint
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "-", "http://localhost:8733/live"]
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      postgres:
        condition: service_healthy
      hatchet-rabbitmq:
        condition: service_healthy

  # Hatchet dashboard provides the web UI for workflow management:
  #  - Workflow monitoring and execution visualization
  #  - User authentication and tenant management  
  #  - Dashboard for workflow runs, workers, and system status
  #  - Web interface accessible on port 7274
  hatchet-dashboard:
    image: ghcr.io/hatchet-dev/hatchet/hatchet-dashboard:v0.53.15
    profiles: [step3]
    command: sh ./entrypoint.sh --config /hatchet/config
    restart: on-failure
    environment:
      # Database connection (dashboard needs direct DB access)
      - DATABASE_URL=postgres://${HATCHET_POSTGRES_USER}:${HATCHET_POSTGRES_PASSWORD}@postgres:5432/${HATCHET_POSTGRES_DBNAME}?sslmode=disable
      # RabbitMQ connection  
      - SERVER_TASKQUEUE_RABBITMQ_URL=amqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@hatchet-rabbitmq:5672/
    volumes:
      - hatchet_certs:/hatchet/certs
      - ./config/hatchet:/hatchet/config
    networks:
      - frontend  # User-facing web interface
      - backend   # Needs to communicate with hatchet-engine
      - postgres  # Needs direct database access
    ports:
      - "7274:80"  # Web dashboard interface
    depends_on:
      hatchet-engine:
        condition: service_healthy

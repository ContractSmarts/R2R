[app]
# LLM used for internal operations, like deriving conversation names
fast_llm = "azure/gpt-4.1-mini"

# LLM used for user-facing output, like RAG replies
quality_llm = "azure/gpt-4.1"

# LLM used for processing visual inputs (same as main model)
vlm = "azure/gpt-4.1"

# LLM used for transcription
audio_lm = "azure/whisper-1"

# Research agent models (e.g., multi-step reasoning)
reasoning_llm = "azure/o4-mini"
planning_llm  = "azure/o4-mini"

[embedding]
# Used for document vectorization
base_model = "azure/text-embedding-3-small"
# Optional: explicitly set embedding dimensionality
# base_dimension = 1536 # default for text-embedding-3-small

[completion_embedding]
# Used for embedding LLM completions (e.g., enriched summaries)
base_model = "azure/text-embedding-3-small"

[ingestion]
provider = "unstructured_local"
strategy = "auto"                 # auto-select best parser per file type
chunking_strategy = "by_title"    # keep chunks aligned with sections
new_after_n_chars = 2048          # split chunks when title is far away
max_characters = 8192             # upper limit for any single chunk (8192 for legal documents)
combine_under_n_chars = 1024      # small chunks get merged
overlap = 1024                    # soft overlap between adjacent chunks

document_summary_model = "azure/gpt-4.1-mini"
# enable automatic extraction of entities and relations
automatic_extraction = true

  [ingestion.extra_parsers]
    pdf = ["zerox", "ocr"]

  [ingestion.chunk_enrichment_settings]
    generation_config = { model = "azure/gpt-4.1-mini" }

# ------------------------------------------------------
# [orchestration] section: Hatchet settings
# ------------------------------------------------------
[orchestration]
provider = "hatchet"
## 4 vCPU deployment
kg_creation_concurrency_limit = 6 # 4-6
ingestion_concurrency_limit = 4   # align with vCPU count
kg_concurrency_limit = 4          # tune for knowledge graph if needed

### Orchestration Options:

# ## Original config (large-scale deployment)
# ## Azure F32s_v2, D32s_v5, E32s_v5
# ## 
# # kg_creation_concurrency_limit = 32 # ~32 vCPUs
# # ingestion_concurrency_limit = 4    # 4+ vCPUs
# # kg_concurrency_limit = 8           # 8+ vCPUs
# 
# ## 8 vCPU settings
# # kg_creation_concurrency_limit = 8-12
# # ingestion_concurrency_limit = 4-6
# # kg_concurrency_limit = 6-8
# 
# ## 4 vCPU settings
# # kg_creation_concurrency_limit = 4-6
# # ingestion_concurrency_limit = 3-4
# # kg_concurrency_limit = 4
# 
# ## 2 vCPU settings (E2s v6)
# # kg_creation_concurrency_limit = 2
# # ingestion_concurrency_limit = 2
# # kg_concurrency_limit = 2

# ------------------------------------------------------
# [database] section: Enable for rate limiting & quotas
# ------------------------------------------------------
[database]
  [database.limits]
  # Per-R2R global rate limits
  global_per_min = 60           # number of requests per minute allowed
  monthly_limit = 10000         # total requests allowed per month

  [database.route_limits]
  "/v3/retrieval/search" = { route_per_min = 30, monthly_limit = 5000 }

#  # Optional: User-specific rate limiting (by user UUID)
#  [database.user_limits."47e53676-b478-5b3f-a409-234ca2164de5"]
#  global_per_min = 10
#  route_per_min = 5

# ------------------------------------------------------
# [auth] section: Optional; enable for login, access control
# ------------------------------------------------------
[auth]
provider = "r2r"                        # Options: r2r, oidc (experimental)
access_token_lifetime_in_minutes = 60
refresh_token_lifetime_in_days = 7
require_authentication = true          # Enforce login for all endpoints
require_email_verification = false     # Turn on if you're sending emails
default_admin_email = "david@contractsmarts.ai"
default_admin_password = "contractsmartsAdminPwd"
#
## For OIDC (e.g., Auth0, Azure AD) â€” future option
#[auth.oidc]
#issuer_url = "https://login.microsoftonline.com/your-tenant-id/v2.0"
#client_id = "your-client-id"
#client_secret = "your-client-secret"
#redirect_uri = "https://your-app-url/callback"

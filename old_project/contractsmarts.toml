[app]
# LLM used for internal operations (e.g., naming, summaries)
fast_llm = "azure/gpt-4o-mini"

# LLM used for RAG answers and user-facing chats
quality_llm = "azure/gpt-4o"

# LLM used for processing visual inputs (same as main model)
vlm = "azure/gpt-4o"

# LLM used for audio transcription (voice to text)
audio_lm = "azure/whisper-1"

# Research agent models (e.g., multi-step reasoning)
reasoning_llm = "azure/o4-mini"
planning_llm  = "azure/o4-mini"

[embedding]
# Used for document vectorization
base_model = "azure/text-embedding-3-small"
# Optional: explicitly set embedding dimensionality (1536 for text-embedding-3-small)
# base_dimension = 1536
# leave blank for default base_dimension = 1536

[completion_embedding]
# Used for embedding LLM completions (e.g., enriched summaries)
base_model = "azure/text-embedding-3-small"

[ingestion]
provider = "unstructured_local"
# auto-select best parser per file type
strategy = "auto"
# keep chunks aligned with sections
chunking_strategy = "by_title"
# split chunks when title is far away
new_after_n_chars = 2048
# upper limit for any single chunk
max_characters = 4096
# small chunks get merged
combine_under_n_chars = 1024
# soft overlap between adjacent chunks
overlap = 1024

document_summary_model = "azure/gpt-4o-mini"
# enable automatic extraction of entities and relations
automatic_extraction = true

  [ingestion.extra_parsers]
    pdf = ["zerox", "ocr"]

  [ingestion.chunk_enrichment_settings]
    generation_config = { model = "azure/gpt-4o-mini" }

# ------------------------------------------------------
# [orchestration] section: Hatchet settings
# ------------------------------------------------------
[orchestration]
provider = "hatchet"
## 4 vCPU deployment
kg_creation_concurrency_limit = 6
# align with vCPU count
ingestion_concurrency_limit = 4
# align with vCPU count, tune for knowledge graph if needed
kg_concurrency_limit = 4

### Orchestration Options:

# ## Original config (large-scale deployment)
# ## Azure F32s_v2, D32s_v5, E32s_v5
# ## 
# # kg_creation_concurrency_limit = 32 # ~32 vCPUs
# # ingestion_concurrency_limit = 4    # 4+ vCPUs
# # kg_concurrency_limit = 8           # 8+ vCPUs
# 
# ## 8 vCPU settings
# # kg_creation_concurrency_limit = 8-12
# # ingestion_concurrency_limit = 4-6
# # kg_concurrency_limit = 6-8
# 
# ## 4 vCPU settings
# # kg_creation_concurrency_limit = 4-6
# # ingestion_concurrency_limit = 3-4
# # kg_concurrency_limit = 4
# 
# ## 2 vCPU settings (E2s v6)
# # kg_creation_concurrency_limit = 2
# # ingestion_concurrency_limit = 2
# # kg_concurrency_limit = 2

# ------------------------------------------------------
# [database] section: Enable for rate limiting & quotas
# ------------------------------------------------------
#[database]
#  [database.limits]
#  # Per-R2R global rate limits
#  global_per_min = 60           # number of requests per minute allowed
#  monthly_limit = 10000         # total requests allowed per month
#
#  [database.route_limits]
#  "/v3/retrieval/search" = { route_per_min = 30, monthly_limit = 5000 }
#
#  # Optional: User-specific rate limiting (by user UUID)
#  [database.user_limits."47e53676-b478-5b3f-a409-234ca2164de5"]
#  global_per_min = 10
#  route_per_min = 5


# ------------------------------------------------------
# [auth] section: Optional; enable for login, access control
# ------------------------------------------------------
#[auth]
#provider = "r2r"                        # Options: r2r, oidc (experimental)
#access_token_lifetime_in_minutes = 60
#refresh_token_lifetime_in_days = 7
#require_authentication = true          # Enforce login for all endpoints
#require_email_verification = false     # Turn on if you're sending emails
#default_admin_email = "admin@example.com"
#default_admin_password = "change_me_immediately"
#
## For OIDC (e.g., Auth0, Azure AD) â€” future option
#[auth.oidc]
#issuer_url = "https://login.microsoftonline.com/your-tenant-id/v2.0"
#client_id = "your-client-id"
#client_secret = "your-client-secret"
#redirect_uri = "https://your-app-url/callback"

